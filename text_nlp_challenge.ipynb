{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv('data/IMDB Dataset.csv')\n",
    "imdb.columns = ['review', '-sentiment-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>-sentiment-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review -sentiment-\n",
       "0  One of the other reviewers has mentioned that ...    positive\n",
       "1  A wonderful little production. <br /><br />The...    positive\n",
       "2  I thought this was a wonderful way to spend ti...    positive\n",
       "3  Basically there's a family where a little boy ...    negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...    positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   review       50000 non-null  object\n",
      " 1   -sentiment-  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "imdb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "    # tokenize\n",
    "    doc_words = word_tokenize(doc)\n",
    "\n",
    "    # lowercase\n",
    "    doc_lower = [word.lower() for word in doc_words]\n",
    "\n",
    "    # remove stop words\n",
    "    doc_nostop = [token for token in doc_lower if token not in stopwords.words('english')]\n",
    "\n",
    "    # remove punctuation\n",
    "    doc_nopunc = [word for word in doc_nostop if word.isalpha()]\n",
    "\n",
    "    # stem the tokens\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    doc_stem = [stemmer.stem(word) for word in doc_nopunc]\n",
    "\n",
    "    return_doc = ' '.join(doc_stem)\n",
    "\n",
    "    return return_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into positive and negative documents\n",
    "pos_doc = ''\n",
    "neg_doc = ''\n",
    "for index, row in imdb[:2000].iterrows():\n",
    "    if row['-sentiment-'] == 'positive':\n",
    "        pos_doc = pos_doc + row['review']\n",
    "    else:\n",
    "        neg_doc = neg_doc + row['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the reviews\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 2000000\n",
    "pos_doc = nlp(pos_doc)\n",
    "neg_doc = nlp(neg_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences\n",
    "pos_sents = [[sent, \"positive\"] for sent in pos_doc.sents]\n",
    "neg_sents = [[sent, \"negative\"] for sent in neg_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two docs into one DataFrame\n",
    "sentences = pd.DataFrame(pos_sents[:2000] + neg_sents[:2000], columns = [\"text\", \"-sentiment-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>-sentiment-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(One, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(They, are, right, ,, as, this, is, exactly, w...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(/&gt;The, first, thing, that, struck, me, about,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Trust, me, ,, this, is, not, a, show, for, th...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(This, show, pulls, no, punches, with, regards...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text -sentiment-\n",
       "0  (One, of, the, other, reviewers, has, mentione...    positive\n",
       "1  (They, are, right, ,, as, this, is, exactly, w...    positive\n",
       "2  (/>The, first, thing, that, struck, me, about,...    positive\n",
       "3  (Trust, me, ,, this, is, not, a, show, for, th...    positive\n",
       "4  (This, show, pulls, no, punches, with, regards...    positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = \" \".join(\n",
    "        [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop and token.is_alpha])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the BOW data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "bow_sentences = pd.concat([bow_df, sentences[[\"text\", \"-sentiment-\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.9490625\n",
      "\n",
      "Test set score: 0.6875\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9871875\n",
      "\n",
      "Test set score: 0.65625\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.686875\n",
      "\n",
      "Test set score: 0.59\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = bow_sentences['-sentiment-']\n",
    "X = np.array(bow_sentences.drop(['text','-sentiment-'], 1))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF to convert text feature\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, min_df=2, use_idf=True, norm=u'l2', smooth_idf=True)\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "tfidf_sentences = pd.concat([bow_df, sentences[[\"text\", \"-sentiment-\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbot</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviate</th>\n",
       "      <th>abet</th>\n",
       "      <th>abhorrent</th>\n",
       "      <th>abide</th>\n",
       "      <th>abiding</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>ziyi</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zwick</th>\n",
       "      <th>text</th>\n",
       "      <th>-sentiment-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewer mention watch Oz episode hook</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>right exactly happen</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thing strike Oz brutality unflinche scene viol...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trust faint hearted timid</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pull punch regard drug sex violence</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaliyah  abandon  abbot  abbott  abbreviate  abet  abhorrent  abide  \\\n",
       "0        0        0      0       0           0     0          0      0   \n",
       "1        0        0      0       0           0     0          0      0   \n",
       "2        0        0      0       0           0     0          0      0   \n",
       "3        0        0      0       0           0     0          0      0   \n",
       "4        0        0      0       0           0     0          0      0   \n",
       "\n",
       "   abiding  ability  ...  ziyi  zombie  zombies  zone  zoo  zoom  zulu  zwick  \\\n",
       "0        0        0  ...     0       0        0     0    0     0     0      0   \n",
       "1        0        0  ...     0       0        0     0    0     0     0      0   \n",
       "2        0        0  ...     0       0        0     0    0     0     0      0   \n",
       "3        0        0  ...     0       0        0     0    0     0     0      0   \n",
       "4        0        0  ...     0       0        0     0    0     0     0      0   \n",
       "\n",
       "                                                text  -sentiment-  \n",
       "0             reviewer mention watch Oz episode hook     positive  \n",
       "1                               right exactly happen     positive  \n",
       "2  thing strike Oz brutality unflinche scene viol...     positive  \n",
       "3                          trust faint hearted timid     positive  \n",
       "4                pull punch regard drug sex violence     positive  \n",
       "\n",
       "[5 rows x 8178 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.9604166666666667\n",
      "\n",
      "Test set score: 0.673125\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.99\n",
      "\n",
      "Test set score: 0.614375\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.7179166666666666\n",
      "\n",
      "Test set score: 0.584375\n"
     ]
    }
   ],
   "source": [
    "# Analyze the TF-IDF data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = tfidf_sentences['-sentiment-']\n",
    "X = np.array(tfidf_sentences.drop(['text','-sentiment-'], 1))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Word2Vec to convert text feature\n",
    "import gensim\n",
    "\n",
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=0.5,\n",
    "    window=12,\n",
    "    sg=0,\n",
    "    sample=0.001,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-sentiment-</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>reviewer mention watch Oz episode hook</td>\n",
       "      <td>-0.151249</td>\n",
       "      <td>-0.353188</td>\n",
       "      <td>0.065817</td>\n",
       "      <td>-0.032253</td>\n",
       "      <td>0.008561</td>\n",
       "      <td>-0.006188</td>\n",
       "      <td>0.046282</td>\n",
       "      <td>-0.092320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050658</td>\n",
       "      <td>0.053701</td>\n",
       "      <td>0.031562</td>\n",
       "      <td>-0.004698</td>\n",
       "      <td>0.071064</td>\n",
       "      <td>-0.069873</td>\n",
       "      <td>-0.024052</td>\n",
       "      <td>0.033327</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>-0.026365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>right exactly happen</td>\n",
       "      <td>-0.163511</td>\n",
       "      <td>-0.440060</td>\n",
       "      <td>0.133467</td>\n",
       "      <td>-0.057674</td>\n",
       "      <td>-0.025220</td>\n",
       "      <td>-0.060228</td>\n",
       "      <td>0.018460</td>\n",
       "      <td>-0.098982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022171</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>0.033788</td>\n",
       "      <td>0.024046</td>\n",
       "      <td>0.081334</td>\n",
       "      <td>0.045124</td>\n",
       "      <td>-0.010001</td>\n",
       "      <td>0.040297</td>\n",
       "      <td>-0.009040</td>\n",
       "      <td>-0.035455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>thing strike Oz brutality unflinche scene viol...</td>\n",
       "      <td>-0.155503</td>\n",
       "      <td>-0.412025</td>\n",
       "      <td>0.117487</td>\n",
       "      <td>-0.055959</td>\n",
       "      <td>0.007992</td>\n",
       "      <td>-0.043630</td>\n",
       "      <td>0.038388</td>\n",
       "      <td>-0.115279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021589</td>\n",
       "      <td>0.025454</td>\n",
       "      <td>0.032383</td>\n",
       "      <td>0.020385</td>\n",
       "      <td>0.076433</td>\n",
       "      <td>0.017668</td>\n",
       "      <td>-0.025647</td>\n",
       "      <td>0.034134</td>\n",
       "      <td>-0.011445</td>\n",
       "      <td>-0.025262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>trust faint hearted timid</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.427978</td>\n",
       "      <td>0.131881</td>\n",
       "      <td>-0.045207</td>\n",
       "      <td>-0.045937</td>\n",
       "      <td>-0.075598</td>\n",
       "      <td>0.038368</td>\n",
       "      <td>-0.089455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038107</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.030569</td>\n",
       "      <td>0.036344</td>\n",
       "      <td>0.089230</td>\n",
       "      <td>0.052766</td>\n",
       "      <td>-0.033745</td>\n",
       "      <td>0.028816</td>\n",
       "      <td>-0.011960</td>\n",
       "      <td>-0.031888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>pull punch regard drug sex violence</td>\n",
       "      <td>-0.146440</td>\n",
       "      <td>-0.415308</td>\n",
       "      <td>0.122994</td>\n",
       "      <td>-0.074044</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>-0.042702</td>\n",
       "      <td>0.039688</td>\n",
       "      <td>-0.141606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018393</td>\n",
       "      <td>0.010160</td>\n",
       "      <td>0.048882</td>\n",
       "      <td>0.022060</td>\n",
       "      <td>0.050674</td>\n",
       "      <td>0.091039</td>\n",
       "      <td>0.034090</td>\n",
       "      <td>0.018852</td>\n",
       "      <td>-0.069765</td>\n",
       "      <td>-0.042664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  -sentiment-                                               text         0  \\\n",
       "0    positive             reviewer mention watch Oz episode hook -0.151249   \n",
       "1    positive                               right exactly happen -0.163511   \n",
       "2    positive  thing strike Oz brutality unflinche scene viol... -0.155503   \n",
       "3    positive                          trust faint hearted timid -0.156114   \n",
       "4    positive                pull punch regard drug sex violence -0.146440   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0 -0.353188  0.065817 -0.032253  0.008561 -0.006188  0.046282 -0.092320  ...   \n",
       "1 -0.440060  0.133467 -0.057674 -0.025220 -0.060228  0.018460 -0.098982  ...   \n",
       "2 -0.412025  0.117487 -0.055959  0.007992 -0.043630  0.038388 -0.115279  ...   \n",
       "3 -0.427978  0.131881 -0.045207 -0.045937 -0.075598  0.038368 -0.089455  ...   \n",
       "4 -0.415308  0.122994 -0.074044  0.048729 -0.042702  0.039688 -0.141606  ...   \n",
       "\n",
       "         90        91        92        93        94        95        96  \\\n",
       "0 -0.050658  0.053701  0.031562 -0.004698  0.071064 -0.069873 -0.024052   \n",
       "1 -0.022171  0.023984  0.033788  0.024046  0.081334  0.045124 -0.010001   \n",
       "2 -0.021589  0.025454  0.032383  0.020385  0.076433  0.017668 -0.025647   \n",
       "3 -0.038107  0.005767  0.030569  0.036344  0.089230  0.052766 -0.033745   \n",
       "4  0.018393  0.010160  0.048882  0.022060  0.050674  0.091039  0.034090   \n",
       "\n",
       "         97        98        99  \n",
       "0  0.033327  0.036499 -0.026365  \n",
       "1  0.040297 -0.009040 -0.035455  \n",
       "2  0.034134 -0.011445 -0.025262  \n",
       "3  0.028816 -0.011960 -0.031888  \n",
       "4  0.018852 -0.069765 -0.042664  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_arr = np.zeros((sentences.shape[0], 100))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
    "\n",
    "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
    "w2v_sentences = pd.concat([sentences[[\"-sentiment-\", \"text\"]],word2vec_arr], axis=1)\n",
    "w2v_sentences.dropna(inplace=True)\n",
    "\n",
    "w2v_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.5570698466780238\n",
      "\n",
      "Test set score: 0.5549169859514687\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9948892674616695\n",
      "\n",
      "Test set score: 0.558109833971903\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.8343270868824532\n",
      "\n",
      "Test set score: 0.5498084291187739\n"
     ]
    }
   ],
   "source": [
    "# Analyze the Word2Vec data\n",
    "Y = w2v_sentences['-sentiment-']\n",
    "X = np.array(w2v_sentences.drop(['text','-sentiment-'], 1))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
