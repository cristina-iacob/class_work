{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this module, you learned the basics of deep learning and the fundamental architecture of artificial neural networks. During the examples in the checkpoints, you used a MNIST dataset. In this challenge, you'll work with another dataset: fashion MNIST. Using this dataset, do the following:\n",
    "\n",
    "* Preprocess your data so that you can feed it into ANN models.\n",
    "\n",
    "* Split your data into training and test sets.\n",
    "\n",
    "* Try different ANN models and train them on your training set. You can play with the following:\n",
    "\n",
    "    * Number of layers\n",
    "    * Activation functions of the layers\n",
    "    * Number of neurons in the layers\n",
    "    * Different batch sizes during training\n",
    "\n",
    "* Compare your models' training scores and interpret your results.\n",
    "\n",
    "* Evaluate how your models perform on your test set. Compare the results of your models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Preprocess your data so that you can feed it into ANN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "input_dim = 784  # 28*28\n",
    "output_dim = nb_classes = 10\n",
    "batch_size = 128\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Split your data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, input_dim)\n",
    "X_test = X_test.reshape(10000, input_dim)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = to_categorical(y_train, nb_classes)\n",
    "Y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Try different ANN models and train them on your training set. You can play with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Change number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model with 3 layers\n",
    "model_a1 = Sequential()\n",
    "\n",
    "model_a1.add(Dense(128, input_shape=X_train[0].shape, activation=\"relu\"))\n",
    "model_a1.add(Dense(64, activation=\"relu\"))\n",
    "model_a1.add(Dense(output_dim, activation=\"softmax\"))\n",
    "model_a1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 1.2651 - accuracy: 0.6818\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.5231 - accuracy: 0.8649\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3981 - accuracy: 0.8909s - loss: 0.4\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.3464 - accuracy: 0.9030\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.3157 - accuracy: 0.9111\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.2936 - accuracy: 0.9167\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.2764 - accuracy: 0.9213\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.2621 - accuracy: 0.9251\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.2497 - accuracy: 0.9289\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2388 - accuracy: 0.9320\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2291 - accuracy: 0.9349\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2203 - accuracy: 0.9380\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2121 - accuracy: 0.9395\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2045 - accuracy: 0.9419\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.1974 - accuracy: 0.9437\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1911 - accuracy: 0.9452\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1849 - accuracy: 0.9475s - loss: 0.1861 - accuracy: \n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1793 - accuracy: 0.9487\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1738 - accuracy: 0.9503\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1687 - accuracy: 0.9520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc3a0255cd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_a1.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model_a1.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 130,058\n",
      "Trainable params: 130,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model with 5 layers\n",
    "model_a2 = Sequential()\n",
    "\n",
    "model_a2.add(Dense(128, input_shape=X_train[0].shape, activation=\"relu\"))\n",
    "model_a2.add(Dense(128, activation=\"relu\"))\n",
    "model_a2.add(Dense(64, activation=\"relu\"))\n",
    "model_a2.add(Dense(64, activation=\"relu\"))\n",
    "model_a2.add(Dense(output_dim, activation=\"softmax\"))\n",
    "model_a2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 1.5848 - accuracy: 0.5250\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.5031 - accuracy: 0.8596\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.3518 - accuracy: 0.8989\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.2975 - accuracy: 0.9138\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.2637 - accuracy: 0.9240\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2387 - accuracy: 0.9304\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2191 - accuracy: 0.9359\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2034 - accuracy: 0.9412\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.1898 - accuracy: 0.9442\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.1775 - accuracy: 0.9484\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.1681 - accuracy: 0.9515\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.1586 - accuracy: 0.9535\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.1507 - accuracy: 0.9558\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.1429 - accuracy: 0.9582\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.1363 - accuracy: 0.9602\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.1297 - accuracy: 0.9620\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.1236 - accuracy: 0.9639\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.1184 - accuracy: 0.9653\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.1128 - accuracy: 0.9663\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.1081 - accuracy: 0.9677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc3a030de90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_a2.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model_a2.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Change the activation funtion of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model with the tanh activation function\n",
    "model_b1 = Sequential()\n",
    "\n",
    "model_b1.add(Dense(128, input_shape=X_train[0].shape, activation=\"tanh\"))\n",
    "model_b1.add(Dense(64, activation=\"tanh\"))\n",
    "model_b1.add(Dense(10, activation=\"softmax\"))\n",
    "model_b1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.0538 - accuracy: 0.7437\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.5231 - accuracy: 0.8704\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.4232 - accuracy: 0.8886\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3764 - accuracy: 0.8978\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3476 - accuracy: 0.9044\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3270 - accuracy: 0.9088\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3113 - accuracy: 0.9124\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2985 - accuracy: 0.9163\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2876 - accuracy: 0.9190\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2779 - accuracy: 0.9215\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.2694 - accuracy: 0.9235\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2616 - accuracy: 0.9259\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.2545 - accuracy: 0.9279\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2476 - accuracy: 0.9296\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.2413 - accuracy: 0.9315\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2352 - accuracy: 0.9333\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2294 - accuracy: 0.9353\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2240 - accuracy: 0.9366\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.2186 - accuracy: 0.9376\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2134 - accuracy: 0.9394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc3807eeb90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_b1.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model_b1.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model with the sigmoid activation function\n",
    "model_b2 = Sequential()\n",
    "\n",
    "model_b2.add(Dense(128, input_shape=X_train[0].shape, activation=\"sigmoid\"))\n",
    "model_b2.add(Dense(64, activation=\"sigmoid\"))\n",
    "model_b2.add(Dense(10, activation=\"softmax\"))\n",
    "model_b2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 2.2914 - accuracy: 0.1436\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 2.2314 - accuracy: 0.3118\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 2.1655 - accuracy: 0.4247\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 2.0729 - accuracy: 0.4807\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 1.9428 - accuracy: 0.5236\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7769 - accuracy: 0.5764\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 1.5958 - accuracy: 0.6253\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 1.4238 - accuracy: 0.6798\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 1.2726 - accuracy: 0.7190\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 1.1434 - accuracy: 0.7491\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 1.0344 - accuracy: 0.7699\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9435 - accuracy: 0.7876\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.8678 - accuracy: 0.8030\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.8047 - accuracy: 0.8135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.7516 - accuracy: 0.8226\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.7067 - accuracy: 0.8309\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6683 - accuracy: 0.8380\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6353 - accuracy: 0.8444\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6065 - accuracy: 0.8501\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.5813 - accuracy: 0.8552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc3808eead0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_b2.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model_b2.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Change the number of neurons in the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 235,146\n",
      "Trainable params: 235,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model with more neurons\n",
    "model_c1 = Sequential()\n",
    "\n",
    "model_c1.add(Dense(256, input_shape=X_train[0].shape, activation=\"relu\"))\n",
    "model_c1.add(Dense(128, activation=\"relu\"))\n",
    "model_c1.add(Dense(10, activation=\"softmax\"))\n",
    "model_c1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: 1.1865 - accuracy: 0.7108\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.4794 - accuracy: 0.8728\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.3731 - accuracy: 0.8956\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.3273 - accuracy: 0.9075\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2984 - accuracy: 0.9147\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2772 - accuracy: 0.9213\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2602 - accuracy: 0.9260s - loss: 0.260\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.2460 - accuracy: 0.9308\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2335 - accuracy: 0.9336\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2228 - accuracy: 0.9370\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2132 - accuracy: 0.9395\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2044 - accuracy: 0.9429\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1965 - accuracy: 0.9447\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1891 - accuracy: 0.9468\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1824 - accuracy: 0.9484\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1762 - accuracy: 0.9501\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1702 - accuracy: 0.9516\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1649 - accuracy: 0.9532\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1596 - accuracy: 0.9549\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1546 - accuracy: 0.9563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc390c65990>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_c1.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model_c1.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 1,068,810\n",
      "Trainable params: 1,068,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model with lots more neurons\n",
    "model_c2 = Sequential()\n",
    "\n",
    "model_c2.add(Dense(1024, input_shape=X_train[0].shape, activation=\"relu\"))\n",
    "model_c2.add(Dense(256, activation=\"relu\"))\n",
    "model_c2.add(Dense(10, activation=\"softmax\"))\n",
    "model_c2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1.0531 - accuracy: 0.7709\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.4299 - accuracy: 0.8889\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.3446 - accuracy: 0.9050\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.3046 - accuracy: 0.9150\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2787 - accuracy: 0.9220\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2588 - accuracy: 0.9276\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2426 - accuracy: 0.9318\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2284 - accuracy: 0.9358\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2164 - accuracy: 0.9394\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2057 - accuracy: 0.9425\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1962 - accuracy: 0.9448\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1870 - accuracy: 0.9472\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1793 - accuracy: 0.9498\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1717 - accuracy: 0.9520\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1649 - accuracy: 0.9541\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1586 - accuracy: 0.9555\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1528 - accuracy: 0.9574\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1472 - accuracy: 0.9585\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1420 - accuracy: 0.9603\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1370 - accuracy: 0.9620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc3b5f60fd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_c2.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model_c2.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Change the training batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model with batch size of 64\n",
    "model_d1 = Sequential()\n",
    "\n",
    "model_d1.add(Dense(128, input_shape=X_train[0].shape, activation=\"relu\"))\n",
    "model_d1.add(Dense(64, activation=\"relu\"))\n",
    "model_d1.add(Dense(10, activation=\"softmax\"))\n",
    "model_d1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s 19us/sample - loss: 0.9261 - accuracy: 0.7567\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.3784 - accuracy: 0.8946\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.3096 - accuracy: 0.9131\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.2732 - accuracy: 0.9219\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.2473 - accuracy: 0.9291\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.2269 - accuracy: 0.9342\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.2102 - accuracy: 0.9398\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1962 - accuracy: 0.9440\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1835 - accuracy: 0.9476\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1722 - accuracy: 0.9508\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1627 - accuracy: 0.9535\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1543 - accuracy: 0.9561\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1464 - accuracy: 0.9579\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1394 - accuracy: 0.9600\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1327 - accuracy: 0.9627\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1271 - accuracy: 0.9638\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1217 - accuracy: 0.9651\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1165 - accuracy: 0.9670\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1120 - accuracy: 0.9680\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1075 - accuracy: 0.9696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc391789e90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_d1.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model_d1.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model with batch size of 32\n",
    "model_d2 = Sequential()\n",
    "\n",
    "model_d2.add(Dense(128, input_shape=X_train[0].shape, activation=\"relu\"))\n",
    "model_d2.add(Dense(64, activation=\"relu\"))\n",
    "model_d2.add(Dense(10, activation=\"softmax\"))\n",
    "model_d2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6310 - accuracy: 0.8369\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.2925 - accuracy: 0.9158\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.2415 - accuracy: 0.9311\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.2077 - accuracy: 0.9403\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.1825 - accuracy: 0.9477\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.1622 - accuracy: 0.9537\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.1463 - accuracy: 0.9584\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.1324 - accuracy: 0.9622\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.1210 - accuracy: 0.9657\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.1117 - accuracy: 0.9684\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.1033 - accuracy: 0.9708\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.0959 - accuracy: 0.9726\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.0896 - accuracy: 0.9748\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.0837 - accuracy: 0.9765\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.0787 - accuracy: 0.9777\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.0740 - accuracy: 0.9796\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.0698 - accuracy: 0.9805\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.0659 - accuracy: 0.9814\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.0625 - accuracy: 0.9827\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.0593 - accuracy: 0.9834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc39172cad0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_d2.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model_d2.fit(X_train, Y_train, batch_size=32, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Compare your models' training scores and interpret your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.16479001728643974\n",
      "Train accuracy: 0.9532667\n",
      "Train score: 0.1020082001067698\n",
      "Train accuracy: 0.96996665\n"
     ]
    }
   ],
   "source": [
    "# Training score of model with 3 layers\n",
    "score = model_a1.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "# Score of model with 5 layers\n",
    "score = model_a2.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model with more layers performed better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.20993710252741973\n",
      "Train accuracy: 0.94045\n",
      "Train score: 0.5694485246419907\n",
      "Train accuracy: 0.8576\n"
     ]
    }
   ],
   "source": [
    "# Training score of model with tanh activation function\n",
    "score = model_b1.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "# Training score of model with sigmoid activation function\n",
    "score = model_b2.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Both of these models performed worse than when using the relu activation function (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.1511257340138157\n",
      "Train accuracy: 0.95675\n",
      "Train score: 0.13326519663259387\n",
      "Train accuracy: 0.96323335\n"
     ]
    }
   ],
   "source": [
    "# Training score of model with more neurons (256, 128, 10)\n",
    "score = model_c1.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "# Training score of model with lots more neurons (1024, 256, 10)\n",
    "score = model_c2.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The models with more neurons performed better, though the processing time was significantly longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.10363129419299463\n",
      "Train accuracy: 0.97\n",
      "Train score: 0.05451198863511284\n",
      "Train accuracy: 0.9856833\n"
     ]
    }
   ],
   "source": [
    "# Training score of model with a smaller batch size (64 vs 128)\n",
    "score = model_d1.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "# Training score of model with an even smaller batch size (32)\n",
    "score = model_d2.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The models with the smaller batch sizes performed much better\n",
    "* The batch size seems to have a much larger effect on the performance than the other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Evaluate how your models perform on your test set. Compare the results of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.16851898211017252\n",
      "Test accuracy: 0.9512\n",
      "Test score: 0.12009563096947969\n",
      "Test accuracy: 0.9646\n"
     ]
    }
   ],
   "source": [
    "# Test score of model with 3 layers\n",
    "score = model_a1.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Test score of model with 5 layers\n",
    "score = model_a2.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Same as the training data, the model with more layers performed better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.21214508360922338\n",
      "Test accuracy: 0.9392\n",
      "Test score: 0.5539725347518921\n",
      "Test accuracy: 0.8622\n"
     ]
    }
   ],
   "source": [
    "# Training score of model with tanh activation function\n",
    "score = model_b1.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Training score of model with sigmoid activation function\n",
    "score = model_b2.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Same as the training data, the model with the relu activation function performed better than the other activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.1562782148063183\n",
      "Test accuracy: 0.9544\n",
      "Test score: 0.13889799484536053\n",
      "Test accuracy: 0.9588\n"
     ]
    }
   ],
   "source": [
    "# Training score of model with more neurons (256, 128, 10)\n",
    "score = model_c1.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Training score of model with lots more neurons (1024, 256, 10)\n",
    "score = model_c2.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Same as the training data, the model with more neurons performed a bit better however, it did not have a significantly larger difference in performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.11894182193242013\n",
      "Test accuracy: 0.9637\n",
      "Test score: 0.08545311895292253\n",
      "Test accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "# Training score of model with a smaller batch size (64 vs 128)\n",
    "score = model_d1.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Training score of model with an even smaller batch size (32)\n",
    "score = model_d2.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As with the training data, the models with the smaller batch sizes performed better\n",
    "* The batch size definitely seems to have a much larger effect on the performance than the other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
